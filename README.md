Archive-IO

---

# Archive-IO (AWS Terraform Project)

Archive-IO is an AWS-based **data archival and validation workflow** deployed with **Terraform**.
It automates the process of archiving data from a source RDBMS into **AWS Athena Iceberg**, validating source vs target data, generating reports, and storing metadata.

The project uses **API Gateway, Lambda, Glue, DynamoDB, Step Functions, S3, and IAM**.

---

## Features

* **API Gateway** â†’ Entry point (`archival_flow` resource)
* **Step Functions** â†’ Orchestrates the archival â†’ validation â†’ reporting flow
* **Glue Jobs**:

  * `archival` â†’ moves data from RDBMS â†’ Iceberg
  * `validation` â†’ checks schema, row count, checksums
  * `purge` â†’ optional purge functionality
* **Lambda Functions**:

  * `archival_flow_trigger` â†’ triggers Step Functions
  * `report_generation` â†’ generates PDF reports, uploads to S3
* **DynamoDB** â†’ Stores archival and purge metadata
* **S3** â†’ Stores archived data and PDF reports
* **IAM Roles** â†’ For Lambda, Glue, and Step Functions
* **CloudWatch Logs** â†’ Centralized logging for all components

---

## ğŸ“‚ Project Structure

```
archive-io/
â”œâ”€â”€ terraform_scripts/          # Terraform IaC for all AWS resources
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â”œâ”€â”€ deploy.sh               # Deployment script (dev/qa/prod)
â”‚   â””â”€â”€ env/                    # Environment-specific variables
â”‚       â”œâ”€â”€ dev.tfvars
â”‚       â”œâ”€â”€ qa.tfvars
â”‚       â””â”€â”€ prod.tfvars
â”‚
â”œâ”€â”€ lambda_functions/           # AWS Lambda source code
â”‚   â”œâ”€â”€ archival_flow_trigger/
â”‚   â”‚   â””â”€â”€ handler.py
â”‚   â””â”€â”€ report_generation/
â”‚       â””â”€â”€ handler.py
â”‚
â”œâ”€â”€ glue_jobs/                  # AWS Glue ETL job scripts
â”‚   â”œâ”€â”€ archival/
â”‚   â”‚   â””â”€â”€ archival_job.py
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ validation_job.py
â”‚   â””â”€â”€ purge/
â”‚       â””â”€â”€ purge_job.py
â”‚
â”œâ”€â”€ input_payloads/             # Sample input payloads for API Gateway
â”‚   â””â”€â”€ archival_input_payload.json

```

---

## âš™ï¸ Prerequisites

* [Terraform](https://developer.hashicorp.com/terraform/downloads)
* [Docker Desktop](https://www.docker.com/products/docker-desktop) (must be **installed and running**)
* AWS CLI configured with proper credentials and access

---

## ğŸ› ï¸ Deployment

1. Navigate to the `terraform_scripts` folder:

```bash
cd terraform_scripts
```

2. Run the deployment script for the target environment (`dev`, `qa`, `prod`):

```bash
./deploy.sh dev
```

3. Terraform will provision:

   * API Gateway
   * Lambda functions
   * Glue Jobs
   * Step Functions
   * DynamoDB tables
   * IAM roles
   * S3 buckets
   * Glue connections

---

## â–¶ï¸ Running the Workflow

1. Go to **API Gateway** in AWS Console.
2. Select the resource `archival` â†’ `POST` method.
3. Use the following payload (sample provided in `input_payloads/archival_input_payload.json`):

```json
{
  "glue_connection": "postgres_connection",
  "legal_hold": false,
  "retention_policy": 6,
  "source_database": "postgres_databse_eeql",
  "source_schema": "postgres_test_schema",
  "source_table": [
    {
      "table_name": "products",
      "condition": "Index < 20",
      "query": "SELECT * FROM postgres_test_schema.products WHERE Index < 20"
    },
    {
      "table_name": "leads",
      "condition": "Index < 50"
    },
    {
      "table_name": "customers"
    }
  ],
  "triggered_by": "Gokul",
  "triggered_by_email": "gokul@example.com"
}

```

4. This triggers:

   * **Step Function `archival_flow`**
   * Runs **Glue Job (archival)** â†’ archives data into Iceberg
   * Runs **Glue Job (validation)** â†’ validates source vs target
   * Runs **Lambda (report_generation)** â†’ creates a PDF in S3
   * Updates **DynamoDB** with metadata
   * Sends logs to **CloudWatch**

---

##  Outputs

* **S3** â†’ Archived data + PDF reports
* **DynamoDB** â†’ Metadata storage
* **CloudWatch** â†’ Logs for Lambda, Glue, Step Functions
* **API Gateway URL** â†’ Entry point for triggering archival workflow

---

## IAM Roles

The following IAM roles are created:

* `lambda_role`
* `glue_role`
* `step_function_role`

Each role has least-privilege access for its respective service.

---

## Environments

* **Development** (`dev`)
* **Quality Assurance** (`qa`)
* **Production** (`prod`)

Switch environments using:

```bash
./deploy.sh dev
```

---

## ğŸ“ Notes

* All infrastructure is fully managed by **Terraform**.
* Logs are centralized in **CloudWatch** under `clpidwatch`.
* The project is modular and supports adding new Glue jobs or Lambda functions easily.

---


