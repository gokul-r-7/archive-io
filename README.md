# archive-io


Got it ğŸ‘ hereâ€™s the **full `README.md` file**, ready for copy-paste into your repo.

---

```markdown
# Archive-IO (AWS Terraform Project)

Archive-IO is an AWS-based **data archival and validation workflow** deployed with **Terraform**.  
It automates the process of archiving data from a source RDBMS into **AWS Athena Iceberg**, validating source vs target data, generating reports, and storing metadata.  

The project uses **API Gateway, Lambda, Glue, DynamoDB, Step Functions, S3, and IAM**.

---

## ğŸš€ Features

- **API Gateway** â†’ Entry point (`archival_flow` resource)
- **Step Functions** â†’ Orchestrates the archival â†’ validation â†’ reporting flow
- **Glue Jobs**:
  - `archival` â†’ moves data from RDBMS â†’ Iceberg
  - `validation` â†’ checks schema, row count, checksums
  - `purge` â†’ optional purge functionality
- **Lambda Functions**:
  - `archival_flow_trigger` â†’ triggers Step Functions
  - `report_generation` â†’ generates PDF reports, uploads to S3
- **DynamoDB** â†’ Stores archival and purge metadata
- **S3** â†’ Stores archived data and PDF reports
- **IAM Roles** â†’ For Lambda, Glue, and Step Functions
- **CloudWatch Logs** â†’ Centralized logging for all components

---

## ğŸ“‚ Project Structure

```

archive-io/
â”œâ”€â”€ terraform_scripts/          # Terraform IaC for all AWS resources
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â”œâ”€â”€ deploy.sh               # Deployment script (dev/qa/prod)
â”‚   â””â”€â”€ env/                    # Environment-specific variables
â”‚       â”œâ”€â”€ dev.tfvars
â”‚       â”œâ”€â”€ qa.tfvars
â”‚       â””â”€â”€ prod.tfvars
â”‚
â”œâ”€â”€ lambda_functions/           # AWS Lambda source code
â”‚   â”œâ”€â”€ archival_flow_trigger/
â”‚   â”‚   â””â”€â”€ handler.py
â”‚   â””â”€â”€ report_generation/
â”‚       â””â”€â”€ handler.py
â”‚
â”œâ”€â”€ glue_jobs/                  # AWS Glue ETL job scripts
â”‚   â”œâ”€â”€ archival/
â”‚   â”‚   â””â”€â”€ archival_job.py
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ validation_job.py
â”‚   â””â”€â”€ purge/
â”‚       â””â”€â”€ purge_job.py
â”‚
â”œâ”€â”€ input_payloads/             # Sample input payloads for API Gateway
â”‚   â””â”€â”€ archival_input_payload.json
â”‚
â””â”€â”€ logs/                       # CloudWatch / debugging logs
â””â”€â”€ clpidwatch/

````

---

## âš™ï¸ Prerequisites

- [Terraform](https://developer.hashicorp.com/terraform/downloads)
- [Docker Desktop](https://www.docker.com/products/docker-desktop) (must be **installed and running**)
- AWS CLI configured with proper credentials and access

---

## ğŸ› ï¸ Deployment

1. Navigate to the `terraform_scripts` folder:

```bash
cd terraform_scripts
````

2. Run the deployment script for the target environment (`dev`, `qa`, `prod`):

```bash
./deploy.sh dev
```

3. Terraform will provision:

   * API Gateway
   * Lambda functions
   * Glue Jobs
   * Step Functions
   * DynamoDB tables
   * IAM roles
   * S3 buckets
   * Glue connections

---

## â–¶ï¸ Running the Workflow

1. Go to **API Gateway** in AWS Console.
2. Select the resource `archival` â†’ `POST` method.
3. Use the following payload (sample provided in `input_payloads/archival_input_payload.json`):

```json
{
  "SourceSystemId": "INT024_TEST_DATA",
  "TargetSystem": "AWS_Athena",
  "ArchiveType": "Full",
  "Tables": ["EMPLOYEES", "SALARIES"]
}
```

4. This triggers:

   * **Step Function `archival_flow`**
   * Runs **Glue Job (archival)** â†’ archives data into Iceberg
   * Runs **Glue Job (validation)** â†’ validates source vs target
   * Runs **Lambda (report_generation)** â†’ creates a PDF in S3
   * Updates **DynamoDB** with metadata
   * Sends logs to **CloudWatch**

---

## ğŸ“Š Outputs

* **S3** â†’ Archived data + PDF reports
* **DynamoDB** â†’ Metadata storage
* **CloudWatch** â†’ Logs for Lambda, Glue, Step Functions
* **API Gateway URL** â†’ Entry point for triggering archival workflow

---

## ğŸ”’ IAM Roles

The following IAM roles are created:

* `lambda_role`
* `glue_role`
* `step_function_role`

Each role has least-privilege access for its respective service.

---

## ğŸŒ Environments

* **Development** (`dev`)
* **Quality Assurance** (`qa`)
* **Production** (`prod`)

Switch environments using:

```bash
./deploy.sh qa
```

---

## ğŸ“ Notes

* All infrastructure is fully managed by **Terraform**.
* Logs are centralized in **CloudWatch** under `clpidwatch`.
* The project is modular and supports adding new Glue jobs or Lambda functions easily.

---

## ğŸ‘¤ Author

**Gokul R.**

```

---

âœ… This is a **ready-to-use README.md** that matches your existing folder structure.  

Do you also want me to include an **architecture diagram (workflow picture)** section in the README, so it looks more like an open-source project?
```
